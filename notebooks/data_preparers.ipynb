{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Configuration Format of Text\n",
    "The initial format, at least for workability will need to be a list of words and punctuation. Getting into this format will be the first step of configuration that the text will undergo as it is simplest when trying to create a dictionary.\n",
    "\n",
    "#### Example Text (Emma):\n",
    "Using the `nltk` library (and for simplicity's sake) we will read in Jane Austen's *Emma*, which can conveniently be loaded in as a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.gutenberg.fileids())\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Emma',\n",
       " 'by',\n",
       " 'Jane',\n",
       " 'Austen',\n",
       " '1816',\n",
       " ']',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " ',',\n",
       " 'handsome',\n",
       " ',',\n",
       " 'clever',\n",
       " ',',\n",
       " 'and',\n",
       " 'rich']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 20 words/punctuations of emma\n",
    "emma[0: 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Once we have the corpus in a list, we need to provide some preprocessing options that can prepare the data for being loaded up into a neural net. Again, these are just options and could potentially improve performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus_list, lowercase=False, stopwords_list=None):\n",
    "    \"\"\"Preprocess the Corpus List.\n",
    "    \n",
    "    Provides some simple preprocessing steps that could be beneficial for training purposes. This includes \n",
    "    an option to lowercase all words in the corpus list and to remove stopwords.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus_list: list\n",
    "        List of words in the corpus.\n",
    "        \n",
    "    lowercase: bool\n",
    "        Option to lowercase all words int he corpus_list.\n",
    "        \n",
    "    stopwords_list: list\n",
    "        List of stopwords to remove from the corpus_list.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    corpus_list: list\n",
    "        The preprocessed corpus_list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # lowercases words in corpus\n",
    "    if lowercase:\n",
    "        corpus_list = [w.lower() for w in corpus_list]\n",
    "    \n",
    "    # removes stopwords\n",
    "    if stopwords_list is not None:\n",
    "        corpus_list = [w for w in corpus_list if w not in stopwords_list]\n",
    "        \n",
    "    return corpus_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preped = preprocess(emma[0:20], lowercase=True, stopwords_list= ['emma', 'volume'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary Dictionary\n",
    "Neural nets don't take text inputs so we need to convert the words into integers that act as key references back to the words. `create_dictionary` is this step in the process and provides a dictionary where the word acts as the key and the integer is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(preprocessed_list):\n",
    "    \"\"\"Create a Vocabulary Dictionary.\n",
    "    \n",
    "    Create a dictionary of the vocab from a list of words in a corpus. This function\n",
    "    all so provides the option to preprocess on the fly.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    preprocessed_list: list\n",
    "        List of preprocessed words in the corpus.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    vocab_dict: tuple\n",
    "        A vocabulary dictionary => {word: int} and a reverse dictionary => {int: word}.\n",
    "    \"\"\"\n",
    " \n",
    "    uniq_words = list(set(preprocessed_list))\n",
    "    word_indexes = list(range(0, len(uniq_words)))\n",
    "    \n",
    "    vocab_dict = dict(zip(uniq_words, word_indexes))\n",
    "    reverse_dict = dict(zip(word_indexes, uniq_words))\n",
    "    \n",
    "    return vocab_dict, reverse_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = create_dictionary(preped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Since the neural net can't take text, it is necessary to map the text to a vector of word integers corresponding to the `vocab_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_list(preprocessed_list, vocab_dict):\n",
    "    \"\"\"Encode Vocabulary List.\n",
    "    \n",
    "    Encodes the preprocessed text using the vocabulary dict.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    preprocessed_list: list\n",
    "        The preprocessed words int he corpus.\n",
    "        \n",
    "    vocab_dict: dict\n",
    "        The vocabulary dict => {word: int}.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    encoded_list: list\n",
    "        The encoded version of the text list.\n",
    "    \"\"\"\n",
    "    encoded_list = [vocab_dict[w] for w in preprocessed_list]\n",
    "    \n",
    "    return encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encode_list(preped, vocabs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Arrays\n",
    "The next step is to configure the encoded list into a 2-dimensional array in which the number of inputs can be specified. Since the purpose of this particular application is to create text, each row represents a sequence of `n` words where the `n + 1` word is the target. The next row is then just shifted by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_arrays(encoded_list, num_inputs):\n",
    "    \"\"\"Configure the Encoded Data into a 2-dimensional Array\n",
    "\n",
    "    Creates a 2-dimensional array from the encoded data of an arbitrary number of inputs. Each row \n",
    "    contains `num_inputs` + 1 values where the last value in each row represents the target value and\n",
    "    the ones previous the inputs. These are just shifting rows where the first `num_inputs` are the first \n",
    "    inputs in row 1 and then for row 2 the row shifts over by 1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    encoded_list: list\n",
    "        A list of encoded words.\n",
    "        \n",
    "    num_inputs: int\n",
    "        The number of input values per row.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple of a 2-dimensional numpy.array representative of the inputs and a 1-dimensional \n",
    "        numpy.array representing the targets.\n",
    "    \"\"\"\n",
    "    config_lists_X = []\n",
    "    config_lists_y = []\n",
    "    for i in range(0, len(encoded_list) - num_inputs):\n",
    "        config_lists_X.append(list(encoded_list[i: i + num_inputs]))\n",
    "        config_lists_y.append(encoded_list[i + num_inputs])\n",
    "        \n",
    "    return np.array(config_lists_X), np.array(config_lists_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2, 11, 13],\n",
       "        [11, 13, 12],\n",
       "        [13, 12,  1],\n",
       "        [12,  1,  3],\n",
       "        [ 1,  3,  6],\n",
       "        [ 3,  6,  7],\n",
       "        [ 6,  7,  6],\n",
       "        [ 7,  6,  4],\n",
       "        [ 6,  4,  8],\n",
       "        [ 4,  8,  0],\n",
       "        [ 8,  0,  8],\n",
       "        [ 0,  8,  5],\n",
       "        [ 8,  5,  8],\n",
       "        [ 5,  8, 10]]),\n",
       " array([12,  1,  3,  6,  7,  6,  4,  8,  0,  8,  5,  8, 10,  9]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configure_arrays(encoded, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodingProletariat(object):\n",
    "    def __init__(self, corpus_list, num_inputs, lowercase=False, stopwords_list=None):\n",
    "        self.corpus_list = corpus_list\n",
    "        self.num_inputs = num_inputs\n",
    "        self.preprocessed = self._preprocess(self.corpus_list, lowercase=lowercase, stopwords_list=None)\n",
    "        self.vocab_dict, self.reverse_dict = self._create_dictionary(self.preprocessed)\n",
    "        self.encoded_list = self._encode_list(self.preprocessed, self.vocab_dict)\n",
    "        self.encodings = self._configure_arrays(self.encoded_list, self.num_inputs)\n",
    "        \n",
    "    def _preprocess(self, corpus_list, lowercase=False, stopwords_list=None):\n",
    "        \"\"\"Preprocess the Corpus List.\n",
    "    \n",
    "        Provides some simple preprocessing steps that could be beneficial for training purposes. This includes \n",
    "        an option to lowercase all words in the corpus list and to remove stopwords.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus_list: list\n",
    "            List of words in the corpus.\n",
    "            \n",
    "        lowercase: bool\n",
    "            Option to lowercase all words int he corpus_list.\n",
    "            \n",
    "        stopwords_list: list\n",
    "            List of stopwords to remove from the corpus_list.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        corpus_list: list\n",
    "            The preprocessed corpus_list.\n",
    "        \"\"\"\n",
    "    \n",
    "        # lowercases words in corpus\n",
    "        if lowercase:\n",
    "            corpus_list = [w.lower() for w in corpus_list]\n",
    "        \n",
    "        # removes stopwords\n",
    "        if stopwords_list is not None:\n",
    "            corpus_list = [w for w in corpus_list if w not in stopwords_list]\n",
    "            \n",
    "        return corpus_list\n",
    "    \n",
    "    def _create_dictionary(self ,preprocessed_list):\n",
    "        \"\"\"Create a Vocabulary Dictionary.\n",
    "        \n",
    "        Create a dictionary of the vocab from a list of words in a corpus. This function\n",
    "        all so provides the option to preprocess on the fly.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        preprocessed_list: list\n",
    "            List of preprocessed words in the corpus.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        vocab_dict: tuple\n",
    "            A vocabulary dictionary => {word: int} and a reverse dictionary => {int: word}.\n",
    "        \"\"\"\n",
    "     \n",
    "        uniq_words = list(set(preprocessed_list))\n",
    "        word_indexes = list(range(0, len(uniq_words)))\n",
    "        \n",
    "        vocab_dict = dict(zip(uniq_words, word_indexes))\n",
    "        reverse_dict = dict(zip(word_indexes, uniq_words))\n",
    "        \n",
    "        return vocab_dict, reverse_dict\n",
    "    \n",
    "    def _encode_list(self, preprocessed_list, vocab_dict):\n",
    "        \"\"\"Encode Vocabulary List.\n",
    "        \n",
    "        Encodes the preprocessed text using the vocabulary dict.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        preprocessed_list: list\n",
    "            The preprocessed words int he corpus.\n",
    "            \n",
    "        vocab_dict: dict\n",
    "            The vocabulary dict => {word: int}.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        encoded_list: list\n",
    "            The encoded version of the text list.\n",
    "        \"\"\"\n",
    "        encoded_list = [vocab_dict[w] for w in preprocessed_list]\n",
    "        \n",
    "        return encoded_list\n",
    "    \n",
    "    def _configure_arrays(self, encoded_list, num_inputs):\n",
    "        \"\"\"Configure the Encoded Data into a 2-dimensional Array\n",
    "    \n",
    "        Creates a 2-dimensional array from the encoded data of an arbitrary number of inputs. Each row \n",
    "        contains `num_inputs` + 1 values where the last value in each row represents the target value and\n",
    "        the ones previous the inputs. These are just shifting rows where the first `num_inputs` are the first \n",
    "        inputs in row 1 and then for row 2 the row shifts over by 1.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoded_list: list\n",
    "            A list of encoded words.\n",
    "            \n",
    "        num_inputs: int\n",
    "            The number of input values per row.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            A 2-dimensional array.\n",
    "        \"\"\"\n",
    "        config_lists = []\n",
    "        for i in range(0, len(encoded_list) - num_inputs):\n",
    "            config_lists.append(list(encoded_list[i: i + (num_inputs + 1)]))\n",
    "            \n",
    "        return np.array(config_lists)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4920, 1480, 3040],\n",
       "       [1480, 3040, 1822],\n",
       "       [3040, 1822, 5741],\n",
       "       ...,\n",
       "       [5378, 6412,  588],\n",
       "       [6412,  588, 3770],\n",
       "       [ 588, 3770, 4959]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = EncodingProletariat(emma, num_inputs=2, lowercase=True, stopwords_list=['chapter'])\n",
    "e.encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next bit of information is simply just a print message to keep track of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ________\n",
      "{\\__/}||E: 10  |\n",
      "(• ,•)||_______|\n",
      "/ > />||\n"
     ]
    }
   ],
   "source": [
    "print(\"       ________\\n{\\\\__/}||E: 10  |\\n(• ,•)||_______|\\n/ > />||\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
